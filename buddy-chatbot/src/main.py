# src/main.py
import streamlit as st
import re # â˜…è¿½åŠ : ã“ã‚ŒãŒãªã„ã¨ã‚³ãƒ¼ãƒ‰ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“
from characters import DATA as CHARACTERS

# åˆ†å‰²ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules import ui, brain, voice

# --- 1. åˆæœŸè¨­å®š ---
ui.setup_page()

if "histories" not in st.session_state:
    st.session_state["histories"] = {name: [] for name in CHARACTERS.keys()}
    if "current_char_name" not in st.session_state:
        st.session_state["current_char_name"] = list(CHARACTERS.keys())[0]

# --- 2. ã‚µã‚¤ãƒ‰ãƒãƒ¼è¡¨ç¤º ---
selected_name, is_analysis_mode, uploaded_file = ui.display_sidebar(
    CHARACTERS, 
    st.session_state["current_char_name"], 
    st.session_state["histories"]
)
st.session_state["current_char_name"] = selected_name

current_char = CHARACTERS[selected_name]
current_history = st.session_state["histories"][selected_name]
char_index = list(CHARACTERS.keys()).index(selected_name)

# --- 3. ãƒ¡ã‚¤ãƒ³ç”»é¢ãƒ˜ãƒƒãƒ€ãƒ¼ ---
ui.display_main_header(selected_name, current_char, is_analysis_mode)

# --- 4. ä¼šè©±ãƒ­ã‚°è¡¨ç¤º ---
ui.display_chat_history(current_history, current_char['icon'])

# --- 5. å…¥åŠ›ã‚¨ãƒªã‚¢ ---
st.write("---")
col_mic, col_spacer = st.columns([2, 8])
with col_mic:
    st.write("ğŸ™ï¸ éŸ³å£°å…¥åŠ›:")
    audio_data = voice.get_audio_input(char_index)

text_input = st.chat_input(f"{selected_name} ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹...")

# --- 6. å‡¦ç†å®Ÿè¡Œ ---
if audio_data or text_input:
    # (ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›è¡¨ç¤º...çœç•¥...å‰ã¨åŒã˜)
    display_text = "ğŸ¤ (éŸ³å£°)" if audio_data else text_input
    if is_analysis_mode and uploaded_file: display_text += f" + ğŸ“{uploaded_file.name}"
    elif not is_analysis_mode and uploaded_file: display_text += " + ğŸ“¸"
    
    st.chat_message("user").write(display_text)
    st.session_state["histories"][selected_name].append({"role": "user", "content": display_text})

    # AIå‡¦ç†
    with st.spinner(f"{selected_name} ãŒè€ƒãˆä¸­..."):
        ai_msg = brain.get_gemini_response(
            prompt=text_input if text_input else "éŸ³å£°å…¥åŠ›",
            image=uploaded_file if not is_analysis_mode else None,
            csv_file=uploaded_file if is_analysis_mode else None,
            audio_bytes=audio_data['bytes'] if audio_data else None,
            system_prompt=current_char['prompt']
        )
        
        # 1. AIã®å›ç­”ã‚’è¡¨ç¤º
        st.chat_message("assistant", avatar=current_char['icon']).write(ai_msg)
        st.session_state["histories"][selected_name].append({"role": "assistant", "content": ai_msg})
        
        # 2. â˜…Phase 5: ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ & ã‚°ãƒ©ãƒ•æç”»
        code_match = re.search(r"```python\n(.*?)```", ai_msg, re.DOTALL)
        
        if code_match:
            python_code = code_match.group(1)
            
            with st.status("ğŸ‘©â€ğŸ’» FuuãŒã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œä¸­...", expanded=True):
                st.write("ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰:")
                st.code(python_code, language='python')
                
                # â˜…ä¿®æ­£: æˆ»ã‚Šå€¤ã‚’2ã¤å—ã‘å–ã‚‹ (ãƒ†ã‚­ã‚¹ãƒˆ, ã‚°ãƒ©ãƒ•ç”»åƒ)
                result_text, result_fig = brain.execute_python_code(python_code)
                
                st.write("å®Ÿè¡Œçµæœ:")
                st.info(result_text)
                
                # çµæœãƒ­ã‚°ä½œæˆ
                log_content = f"\nã€ã‚·ã‚¹ãƒ†ãƒ é€šçŸ¥: ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œçµæœã€‘\n{result_text}"

                # â˜…ã‚°ãƒ©ãƒ•ãŒã‚ã‚Œã°è¡¨ç¤ºï¼
                if result_fig:
                    st.write("ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•:")
                    st.pyplot(result_fig) # ã“ã“ã§æç”»ï¼
                    log_content += "\n(ã‚°ãƒ©ãƒ•ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ)"
                
                st.session_state["histories"][selected_name].append({"role": "system", "content": log_content})

    # éŸ³å£°å†ç”Ÿ
    voice_bytes = voice.play_voice(ai_msg)
    if voice_bytes:
        st.audio(voice_bytes, format="audio/wav", autoplay=True)